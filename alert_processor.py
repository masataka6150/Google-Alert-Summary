import os
import requests
import feedparser
import csv
import time
from urllib.parse import urlparse, parse_qs
from newspaper import Article
from openai import OpenAI
from dotenv import load_dotenv
import pandas as pd
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryBufferMemory


# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

# ğŸ”¹ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹å‹•ã§è¨­å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
alert_keywords = ["å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼", "çœã‚¨ãƒ", "ã‚«ãƒ¼ãƒœãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "ã‚¨ãƒãƒ«ã‚®ãƒ¼æ”¿ç­–"]

# ğŸ”¹ é–¢é€£èªCSVã®ä¿å­˜ãƒ‘ã‚¹
RELATED_TERMS_CSV = "related_terms.csv"

# ğŸ”¹ RSS URLï¼ˆGoogleã‚¢ãƒ©ãƒ¼ãƒˆã‹ã‚‰å–å¾—ï¼‰
RSS_FEED_URL = "https://www.google.com/alerts/feeds/xxxxxxxxxxxxxxxxxxxxxxxx"

# é–¢é€£èªã®æŠ½å‡ºé–¢æ•°
def extract_related_terms(keywords, top_n=10):
    prompt = f"ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹é‡è¦ãªå˜èªã‚’ã€ãã‚Œãã‚Œã«ã¤ã„ã¦2ã¤ãšã¤æŒ™ã’ã¦ãã ã•ã„ã€‚\n\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:\n" + "\n".join(keywords)
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.3
    )
    return response.choices[0].message.content.strip()

# æŠ½å‡ºçµæœã‚’CSVã«ä¿å­˜
def save_terms_to_csv(terms_str):
    terms = []
    for line in terms_str.splitlines():
        if ":" in line:
            keyword, related = line.split(":", 1)
            for word in related.strip().split("ã€"):
                terms.append([keyword.strip(), word.strip()])
    df = pd.DataFrame(terms, columns=["Keyword", "RelatedTerm"])
    df.to_csv(RELATED_TERMS_CSV, index=False, encoding="utf-8")
    return df

# è¨˜äº‹æœ¬æ–‡æŠ½å‡º
def extract_text_from_url(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text.strip()
    except Exception:
        return ""

# é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæ„å‘³çš„ï¼‰
def is_semantically_related(text, keywords_df, original_keywords):
    if text.strip() == "":
        return False
    keyword_block = ", ".join(original_keywords)
    related_terms_block = ", ".join(keywords_df["RelatedTerm"].tolist())
    check_prompt = f"""ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã¨ã€æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword_block}ã€ãŠã‚ˆã³ãã®é–¢é€£èªã€Œ{related_terms_block}ã€ã¨ã®æ„å‘³çš„ãªé–¢é€£æ€§ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
é–¢é€£æ€§ãŒã‚ã‚‹å ´åˆã¯ã€ŒYesã€ã€é–¢é€£æ€§ãŒãªã„å ´åˆã¯ã€ŒNoã€ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹æœ¬æ–‡:
{text[:1500]}"""  # æœ€åˆã®1500æ–‡å­—ã§è©•ä¾¡

    messages = [{"role": "user", "content": check_prompt}]
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0
    )
    result = response.choices[0].message.content.strip().lower()
    return "yes" in result

# è¦ç´„å‡¦ç†ï¼ˆç®‡æ¡æ›¸ãï¼‰
def summarize_text(text):
    prompt = f"""ä»¥ä¸‹ã®æ–‡ç« ã‚’ã€1é …ç›®50ã€œ100æ–‡å­—ã®ç®‡æ¡æ›¸ã3ç‚¹ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚è¦ç´„ã®æ–‡å­—æ•°ã¯å¿…ãš50ã€œ100æ–‡å­—ã®é–“ã«ãŠã•ã‚ã¦ãã ã•ã„ã€‚\n\n{text[:3000]}"""
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.3
    )
    return response.choices[0].message.content.strip()

# URLãƒ‡ã‚³ãƒ¼ãƒ‰å‡¦ç†
def extract_real_url(google_url):
    parsed = urlparse(google_url)
    params = parse_qs(parsed.query)
    return params.get("url", [google_url])[0]

# ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†æœ¬ä½“
def process_alerts():
    print("âœ… é–¢é€£èªã‚’æŠ½å‡ºä¸­...")
    related_terms_str = extract_related_terms(alert_keywords)
    related_df = save_terms_to_csv(related_terms_str)
    print("âœ… æŠ½å‡ºã•ã‚ŒãŸé–¢é€£èª:\n", related_df)

    print("âœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    feed = feedparser.parse(RSS_FEED_URL)

    for entry in feed.entries:
        google_url = entry.link
        real_url = extract_real_url(google_url)
        print(f"\nğŸ”— å‡¦ç†ä¸­: {real_url}")
        text = extract_text_from_url(real_url)
        if not text:
            print("âŒ æœ¬æ–‡å–å¾—å¤±æ•—")
            continue
        print(f"âœ… æœ¬æ–‡å–å¾—æˆåŠŸï¼ˆå†’é ­100å­—ï¼‰: {text[:100]}")

        if is_semantically_related(text, related_df, alert_keywords):
            summary = summarize_text(text)
            print("ğŸ“ è¦ç´„:\n", summary)
        else:
            print("ğŸš« é–¢é€£æ€§ãŒä½ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

import pandas as pd

# alert_processor.py

import pandas as pd

def save_article_summaries_to_csv(articles: list[dict], file_path: str = "article_summaries.csv") -> str:
    """
    è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æ—¥ä»˜ãƒ»URLãƒ»è¦ç´„ã‚’CSVã«ä¿å­˜ã™ã‚‹é–¢æ•°

    Parameters:
        articles (list[dict]): å„è¨˜äº‹ã¯ {"title": ..., "pub_date": ..., "url": ..., "summary": ...} ã‚’å«ã‚€è¾æ›¸
        file_path (str): ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: article_summaries.csvï¼‰

    Returns:
        str: ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    df = pd.DataFrame([
        {
            "ã‚¿ã‚¤ãƒˆãƒ«": a.get("title", ""),
            "æ—¥ä»˜": a.get("pub_date", ""),
            "è¦ç´„": a.get("summary", ""),
            "URL": a.get("url", ""),
        }
        for a in articles
    ])
    df.to_csv(file_path, index=False, encoding="utf-8-sig")
    return file_path



# å®Ÿè¡Œ
if __name__ == "__main__":
    process_alerts()